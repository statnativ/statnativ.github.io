<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://statnativ.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://statnativ.com/" rel="alternate" type="text/html" /><updated>2025-09-07T23:52:20+05:30</updated><id>https://statnativ.com/feed.xml</id><title type="html">Statnativ</title><subtitle>Welcome to Statnativ – Exploring the Intersection of AI, Development, and Systems. We dive deep into AI fundamentals, system design, LLMOps, RAG &amp; Agentic AI, and real-world challenges in scaling products.</subtitle><author><name>Amit Tiwari</name><email>statnativ@gmail.com</email></author><entry><title type="html">Quantization</title><link href="https://statnativ.com/Quantization/" rel="alternate" type="text/html" title="Quantization" /><published>2025-03-02T00:00:00+05:30</published><updated>2025-03-02T00:00:00+05:30</updated><id>https://statnativ.com/Quantization</id><content type="html" xml:base="https://statnativ.com/Quantization/"><![CDATA[<hr />
<p>Title: Quantization
date: 2024-01-02 12:00:00
categories:[AI, Text Processing, Quantization]
tags: [ML, AI, GenAI]
author: “Amit Tiwari”
—</p>

<p><strong>Quantization</strong> is a technique for reducing the model size by compressing the model weights from a <strong>high-precision</strong> value to a <strong>low-precision</strong> value. The weights of a language model are the vectors that can be represented in different data types based on the availability of computational resources and required precision. The default data type of most of the models is a 32-bit floating number (float32), which means each weight of such models takes 4 bytes of space in the memory.</p>

<p><strong>Quantization</strong> reduces the number of bits required for each weight of the model by changing the data type from a 32-bit floating number (float32), which holds more information, to an 8-bit (int8) or a 4-bit integer (int4), which holds less.</p>

<p>Imagine you’re packing for a trip. You’ve got a big suitcase, and you’re stuffing it with clothes that take up a lot of space—like bulky sweaters and thick jeans. This is like a language model’s weights stored as 32-bit floating numbers (float32): they’re detailed and precise, but they take up a lot of room (4 bytes per weight). Now, suppose you realize you need to fit everything into a smaller bag. You start folding your clothes tighter, swapping out the sweaters for thinner shirts, and squeezing things down to essentials. You lose some of the fluffiness and detail, but you still have functional outfits. That’s quantization—shrinking those weights down to something like 8-bit integers (int8, 1 byte) or even 4-bit integers (int4, half a byte), where they take up way less space but still get the job done.</p>

<p>For example, let’s say a weight in a model is originally a float32 value like 3.14159 (a precise number stored in 4 bytes). After quantization to int8, it might get rounded to 3 (a less precise integer stored in just 1 byte). You lose some granularity—like going from a high-def photo to a slightly pixelated one—but the model can still understand and generate text, just with a smaller memory footprint. This is super useful for running models on devices with limited resources, like phones or edge hardware, where you can’t afford to lug around that big suitcase of float32 weights.</p>

<p>Below is a table which clearly explains how the data changes.</p>

<table>
  <tbody>
    <tr>
    </tr>
  </tbody>
</table>

<p>Matrix (float32):</p>

<p>[ 3.14159, -0.7854 ]<br />
[ 2.71828, 1.4142 ]</p>

<table>
  <tbody>
    <tr>
    </tr>
  </tbody>
</table>

<p>Matrix (float16):</p>

<p>[ 3.1416, -0.7854 ]</p>

<p>[ 2.7183,  1.4142 ]</p>

<table>
  <tbody>
    <tr>
    </tr>
  </tbody>
</table>

<p>Matrix (float8):</p>

<p>[ 3.0,   -0.75 ]</p>

<p>[ 2.5,    1.5  ]</p>

<table>
  <tbody>
    <tr>
    </tr>
    <tr>
    </tr>
  </tbody>
</table>

<p>Matrix (int8):</p>

<p>[ 127,  -50 ]</p>

<p>[ 110,   58 ]</p>

<table>
  <tbody>
    <tr>
    </tr>
  </tbody>
</table>

<p>Matrix (int4):</p>

<p>[ 7, -2 ]</p>

<p>[ 6,  3 ]</p>

<table>
  <tbody>
    <tr>
      <td> </td>
    </tr>
  </tbody>
</table>

<p><strong>Why use quantization?</strong>[#]
Quantization is an important technique for fine-tuning and deploying large language models in real-world applications. As the models increase in size and complexity, quantization becomes essential for managing resource consumption without compromising accuracy. The following are some key benefits of using quantization:</p>

<p><strong>Reduced resource consumption:</strong></p>

<p><strong>Fast inference:</strong> Quantization increases the training and inference speed of the model due to less computation and memory required for low-precision weights. This increased speed is beneficial when deploying models in real-time applications.</p>

<p><strong>Increased scalability:</strong> Quantization allows the large-size model to be more scalable without increasing resource requirements. This enables the deployment of the model on various platforms, including hardware-constrained devices.</p>

<p><strong>Lower latency</strong>: Quantization helps reduce latency in real-time applications. Since the model processes information more quickly using low-precision operations, it’s perfect for situations where speed is crucial, like in self-driving cars or language translation.</p>]]></content><author><name>Amit Tiwari</name><email>statnativ@gmail.com</email></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Transformers</title><link href="https://statnativ.com/transformers/" rel="alternate" type="text/html" title="Transformers" /><published>2025-01-02T00:00:00+05:30</published><updated>2025-01-02T00:00:00+05:30</updated><id>https://statnativ.com/transformers</id><content type="html" xml:base="https://statnativ.com/transformers/"><![CDATA[<hr />
<p>title: Transformers
date: 2024-01-02 12:00:00
categories:[AI, Text Processing]
tags: [ML, AI, GenAI]
author: “Dev”
—</p>

<p><img src="/assets/images/attention_research.png" alt="Attention Research" /></p>

<p><strong><em>Text Generation with Transformers: Fine-Tuning for Specific Tasks</em></strong></p>

<p>Transformers have revolutionized Natural Language Processing (NLP), achieving state-of-the-art results in various tasks, including text generation. Unlike recurrent neural networks (RNNs), transformers process entire sequences in parallel, enabling them to capture long-range dependencies more effectively. This blog post dives into fine-tuning a pre-trained transformer model for a specific text generation task.</p>

<p><strong>What are Transformers?</strong></p>

<p>At their core, transformers rely on the mechanism of <em>self-attention</em>. Self-attention allows the model to weigh the importance of different words in a sentence when processing 1 a particular word. This helps the model understand the context and relationships between words.  </p>

<p><a href="https://hackernews-kappa.vercel.app/best/35977891">1. hackernews-kappa.vercel.app</a></p>

<p><a href="https://hackernews-kappa.vercel.app/best/35977891">hackernews-kappa.vercel.app</a></p>

<p>DeepSeek’s transformer architecture introduced several key innovations that differentiate it from standard transformer models like those used in GPT series. Here are the main architectural differences:</p>

<ol>
  <li>
    <p><strong>Multi-Query Attention (MQA)</strong>: Instead of using multiple sets of key and value projections per attention head (as in multi-head attention), DeepSeek uses a shared key and value projection across all heads while maintaining separate query projections. This significantly reduces computational requirements and memory usage while maintaining performance.</p>
  </li>
  <li>
    <p><strong>RoPE (Rotary Positional Embedding)</strong>: DeepSeek employs rotary positional embeddings instead of absolute or learned positional embeddings. RoPE encodes position information directly into the attention computation through a rotation matrix, which helps the model better understand relative positions and extends more effectively to sequences longer than those seen during training.</p>
  </li>
  <li>
    <p><strong>Grouped-Query Attention (GQA)</strong>: An intermediate approach between multi-head and multi-query attention where key and value projections are shared among groups of attention heads rather than completely shared or completely separate. This offers a balance between computational efficiency and representational power.</p>
  </li>
  <li>
    <p><strong>SwiGLU Activation</strong>: DeepSeek uses SwiGLU activation functions in the feed-forward networks instead of traditional ReLU or GELU activations. SwiGLU combines gating mechanisms with swish activations to improve gradient flow and model capacity.</p>
  </li>
  <li>
    <p><strong>Normalization Placement</strong>: DeepSeek implements “pre-normalization” where layer normalization is applied before the self-attention and feed-forward blocks rather than after them. This pre-norm design helps stabilize training for very deep models.</p>
  </li>
  <li>
    <p><strong>Parallel Structure</strong>: The model uses a parallel structure where the residual connections bypass both the attention and feed-forward blocks in parallel rather than in series, which can improve gradient flow during training.</p>
  </li>
  <li>
    <p><strong>Larger Context Windows</strong>: DeepSeek models were trained with significantly larger context windows than earlier models, enabling them to process and reason over much longer sequences of text.</p>
  </li>
</ol>

<p>These architectural choices collectively allow DeepSeek models to achieve high performance while maintaining computational efficiency, especially for long-context reasoning tasks.</p>

<p><strong>Why Fine-Tuning?</strong></p>

<p>Pre-trained transformer models, like BERT, GPT, and RoBERTa, have learned rich language representations from massive datasets. Fine-tuning these models on a smaller, task-specific dataset allows us to adapt their knowledge to our specific needs without training from scratch, which is computationally expensive and requires vast amounts of data.</p>

<p><strong>Fine-Tuning for Text Generation: A Practical Example</strong></p>

<p>Let’s consider the task of generating creative text, specifically short poems. We will use the <code class="language-plaintext highlighter-rouge">transformers</code> library from Hugging Face, which provides easy access to pre-trained models and fine-tuning tools.</p>

<p><strong>1. Installation:</strong></p>

<p>First, install the necessary libraries:</p>

<p>Bash</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install transformers datasets accelerate
</code></pre></div></div>

<p><strong>2. Loading a Pre-trained Model and Tokenizer:</strong></p>

<p>We’ll use GPT-2 as our base model:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
</code></pre></div></div>

<p><strong>3. Preparing the Dataset:</strong></p>

<p>For this example, let’s assume you have a text file (<code class="language-plaintext highlighter-rouge">poems.txt</code>) where each line is a short poem. You’ll need to tokenize the text:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from datasets import load_dataset
</code></pre></div></div>

<p><strong>4. Fine-Tuning the Model:</strong></p>

<p>Now comes the core part: fine-tuning. We’ll use the <code class="language-plaintext highlighter-rouge">Trainer</code> class from the <code class="language-plaintext highlighter-rouge">transformers</code> library:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import TrainingArguments, Trainer
</code></pre></div></div>

<p><strong>5. Generating Text:</strong></p>

<p>After fine-tuning, you can generate new poems:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prompt = "The moon shines bright,"
</code></pre></div></div>

<p><strong>Key Parameters for Generation:</strong></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">max_length</code>: The maximum length of the generated text.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">num_return_sequences</code>: The number of different sequences to generate.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">temperature</code>: Controls the randomness of the generation. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 1.0) make it more creative.</p>
  </li>
</ul>

<p><strong>Conclusion:</strong></p>

<p>Fine-tuning transformers is a powerful technique for adapting pre-trained models to specific text generation tasks. By using the <code class="language-plaintext highlighter-rouge">transformers</code> library, you can easily experiment with different models and datasets to achieve impressive results. Remember to adjust the hyperparameters and training arguments to optimize for your specific use case. This blog post provides a basic introduction; further exploration of advanced techniques like beam search and different decoding strategies can further enhance your text generation capabilities.</p>]]></content><author><name>Amit Tiwari</name><email>statnativ@gmail.com</email></author><summary type="html"><![CDATA[title: Transformers date: 2024-01-02 12:00:00 categories:[AI, Text Processing] tags: [ML, AI, GenAI] author: “Dev” —]]></summary></entry><entry><title type="html">Testing Enhanced Jekyll Script with GitHub Push</title><link href="https://statnativ.com/automation/blogging/testing-enhanced-jekyll-script-with-github-push/" rel="alternate" type="text/html" title="Testing Enhanced Jekyll Script with GitHub Push" /><published>2024-09-07T00:00:00+05:30</published><updated>2024-09-07T00:00:00+05:30</updated><id>https://statnativ.com/automation/blogging/testing-enhanced-jekyll-script-with-github-push</id><content type="html" xml:base="https://statnativ.com/automation/blogging/testing-enhanced-jekyll-script-with-github-push/"><![CDATA[<h1 id="test-github-push-feature">Test GitHub Push Feature</h1>

<p>This is a test blog post to demonstrate the enhanced Jekyll processing script with automatic GitHub push functionality.</p>

<h2 id="features-tested">Features Tested</h2>

<ul>
  <li>✅ <strong>Automatic metadata extraction</strong> from custom format</li>
  <li>✅ <strong>Jekyll frontmatter generation</strong> with proper YAML structure</li>
  <li>✅ <strong>Tag and category organization</strong> for better navigation</li>
  <li>✅ <strong>GitHub integration</strong> with automatic commit and push</li>
  <li>✅ <strong>Content preservation</strong> while cleaning metadata blocks</li>
</ul>

<h2 id="how-it-works">How It Works</h2>

<ol>
  <li><strong>Processing</strong>: The script automatically extracts metadata from the custom format</li>
  <li><strong>Validation</strong>: Ensures all required fields are present</li>
  <li><strong>Conversion</strong>: Creates proper Jekyll post with frontmatter</li>
  <li><strong>Organization</strong>: Updates tag and category systems</li>
  <li><strong>GitHub Push</strong>: Commits changes and pushes to remote repository</li>
</ol>

<h2 id="code-example">Code Example</h2>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Run the enhanced script with GitHub push</span>
<span class="n">ruby</span> <span class="n">process_posts_enhanced</span><span class="p">.</span><span class="nf">rb</span> <span class="o">-</span><span class="nb">p</span>

<span class="c1"># Interactive mode with GitHub push</span>
<span class="n">ruby</span> <span class="n">process_posts_enhanced</span><span class="p">.</span><span class="nf">rb</span> <span class="o">-</span><span class="n">i</span> <span class="o">-</span><span class="nb">p</span>

<span class="c1"># Check git setup</span>
<span class="n">ruby</span> <span class="n">process_posts_enhanced</span><span class="p">.</span><span class="nf">rb</span> <span class="o">-</span><span class="n">g</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>This enhanced script streamlines the entire blog publishing workflow from markdown processing to live deployment on GitHub Pages.</p>

<p>🚀 <strong>Automated with Claude Code</strong></p>]]></content><author><name>StatNativ Team</name></author><category term="automation" /><category term="blogging" /><category term="jekyll" /><category term="github" /><category term="automation" /><category term="claude-code" /><summary type="html"><![CDATA[Test GitHub Push Feature]]></summary></entry><entry><title type="html">HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs</title><link href="https://statnativ.com/machine-learning/hnsw-fast-approximate-nearest-neighbors-with-hierarchical-graphs/" rel="alternate" type="text/html" title="HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs" /><published>2024-02-15T00:00:00+05:30</published><updated>2024-02-15T00:00:00+05:30</updated><id>https://statnativ.com/machine-learning/hnsw-fast-approximate-nearest-neighbors-with-hierarchical-graphs</id><content type="html" xml:base="https://statnativ.com/machine-learning/hnsw-fast-approximate-nearest-neighbors-with-hierarchical-graphs/"><![CDATA[<h1 id="hnsw-fast-approximate-nearest-neighbors-with-hierarchical-graphs">HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs</h1>

<h2 id="1-problem-statement">1. Problem Statement</h2>
<ul>
  <li>Traditional nearest neighbor search becomes computationally expensive in high-dimensional spaces (the “curse of dimensionality”)</li>
  <li>Exact search (like brute-force) is O(n) per query - infeasible for large datasets</li>
  <li>Real-world applications require fast similarity search in recommendation systems, NLP, and image retrieval</li>
</ul>

<hr />

<h2 id="2-solution--approach">2. Solution / Approach</h2>
<p><strong>Hierarchical Navigable Small World (HNSW)</strong> combines:</p>
<ul>
  <li><strong>Probabilistic skip lists</strong> for hierarchical structure</li>
  <li><strong>Small world graphs</strong> for efficient navigation</li>
</ul>

<p><strong>Key mechanisms:</strong></p>
<ol>
  <li><em>Layered structure</em>: Top layers have long connections, bottom layers contain all elements</li>
  <li><em>Greedy search</em>: Start at top layer, move downward while refining search</li>
  <li><em>Heuristic connectivity</em>: Balances exploration and exploitation</li>
</ol>

<h2><img src="https://github.com/statnativ/RAG/raw/statnativ-assets/An-illustration-of-an-HNSW-Graph.png" alt="HNSW Graph Structure" /></h2>
<h2 id="3-code--implementation">3. Code / Implementation</h2>

<h3 id="installation--setup">Installation &amp; Setup</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Install required packages
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">hnswlib</span> <span class="n">groq</span> <span class="n">numpy</span>
</code></pre></div></div>

<h3 id="basic-hnsw-index-construction">Basic HNSW Index Construction</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">hnswlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">groq</span> <span class="kn">import</span> <span class="n">Groq</span>

<span class="c1"># Initialize Groq client (using llama3-70b-8192 for embeddings)
</span><span class="n">client</span> <span class="o">=</span> <span class="n">Groq</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s">"your_groq_api_key"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s">"llama3-70b-8192"</span><span class="p">):</span>
    <span class="s">"""Generate embeddings using Groq API"""</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
        <span class="n">encoding_format</span><span class="o">=</span><span class="s">"float"</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">.</span><span class="n">embedding</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">data</span><span class="p">])</span>

<span class="c1"># Generate sample embeddings
</span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s">"machine learning"</span><span class="p">,</span> <span class="s">"artificial intelligence"</span><span class="p">,</span> <span class="s">"deep learning"</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

<span class="c1"># Initialize HNSW index
</span><span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">hnswlib</span><span class="p">.</span><span class="n">Index</span><span class="p">(</span><span class="n">space</span><span class="o">=</span><span class="s">"cosine"</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

<span class="c1"># Build index
</span><span class="n">index</span><span class="p">.</span><span class="n">init_index</span><span class="p">(</span><span class="n">max_elements</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">ef_construction</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">index</span><span class="p">.</span><span class="n">add_items</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)))</span>

<span class="c1"># Perform search
</span><span class="n">query_embedding</span> <span class="o">=</span> <span class="n">get_embeddings</span><span class="p">([</span><span class="s">"AI technology"</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">labels</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="n">knn_query</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Nearest neighbors: </span><span class="si">{</span><span class="n">labels</span><span class="si">}</span><span class="s"> with distances </span><span class="si">{</span><span class="n">distances</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="complete-implementation">Complete Implementation</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">hnswlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">groq</span> <span class="kn">import</span> <span class="n">Groq</span>

<span class="k">class</span> <span class="nc">HNSWVectorSearch</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s">"llama3-70b-8192"</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">Groq</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s">"your_groq_api_key"</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">create_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
        <span class="s">"""Generate embeddings using Groq API"""</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">client</span><span class="p">.</span><span class="n">embeddings</span><span class="p">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
            <span class="nb">input</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
            <span class="n">encoding_format</span><span class="o">=</span><span class="s">"float"</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">data</span><span class="p">.</span><span class="n">embedding</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">data</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">build_index</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">ef_construction</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
        <span class="s">"""Build HNSW index from text data"""</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">hnswlib</span><span class="p">.</span><span class="n">Index</span><span class="p">(</span><span class="n">space</span><span class="o">=</span><span class="s">"cosine"</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">init_index</span><span class="p">(</span>
            <span class="n">max_elements</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">),</span>
            <span class="n">ef_construction</span><span class="o">=</span><span class="n">ef_construction</span><span class="p">,</span>
            <span class="n">M</span><span class="o">=</span><span class="n">M</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">add_items</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">)))</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="s">"""Search for similar items"""</span>
        <span class="n">query_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_embeddings</span><span class="p">([</span><span class="n">query</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">labels</span><span class="p">,</span> <span class="n">distances</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">knn_query</span><span class="p">(</span><span class="n">query_emb</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">labels</span><span class="p">,</span> <span class="n">distances</span>

<span class="c1"># Usage example
</span><span class="n">text_corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s">"machine learning algorithms"</span><span class="p">,</span>
    <span class="s">"deep neural networks"</span><span class="p">,</span>
    <span class="s">"natural language processing"</span><span class="p">,</span>
    <span class="s">"computer vision applications"</span><span class="p">,</span>
    <span class="s">"reinforcement learning frameworks"</span>
<span class="p">]</span>

<span class="n">searcher</span> <span class="o">=</span> <span class="n">HNSWVectorSearch</span><span class="p">()</span>
<span class="n">searcher</span><span class="p">.</span><span class="n">build_index</span><span class="p">(</span><span class="n">text_corpus</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">.</span><span class="n">search</span><span class="p">(</span><span class="s">"AI models"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Search results: </span><span class="si">{</span><span class="n">results</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>👉 <a href="https://github.com/statnativ/hnsw-implementation-demo">View complete implementation on GitHub</a></p>

<hr />

<h2 id="4-pros-and-cons">4. Pros and Cons</h2>

<table>
  <thead>
    <tr>
      <th>Pros ✅</th>
      <th>Cons ⚠️</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Sublinear search time</td>
      <td>Memory intensive for very large datasets</td>
    </tr>
    <tr>
      <td>High recall accuracy</td>
      <td>Parameter tuning required (M, ef)</td>
    </tr>
    <tr>
      <td>Dynamic index support</td>
      <td>Batch insertion better than incremental</td>
    </tr>
    <tr>
      <td>Multi-threaded support</td>
      <td>Not deterministic for exact reproduction</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="5-key-takeaways">5. Key Takeaways</h2>
<ul>
  <li><strong>Hierarchical structure</strong> enables O(log n) search complexity</li>
  <li><strong>Balance between connectivity</strong> and efficiency through parameters M (neighbors) and ef (search depth)</li>
  <li><strong>Ideal for medium-to-large datasets</strong> where exact search is prohibitive</li>
  <li><a href="https://arxiv.org/abs/1603.09320">Read original paper</a> for theoretical details</li>
</ul>

<hr />

<h2 id="6-future-improvements">6. Future Improvements</h2>
<ul>
  <li>Hybrid quantization techniques for memory reduction</li>
  <li>GPU acceleration for index building</li>
  <li>Automatic parameter tuning based on dataset characteristics</li>
</ul>

<hr />

<h2 id="-checklist">✅ Checklist</h2>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Title, Date, Categories, Tags filled</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Problem clearly defined</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Solution explained with context</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Code snippet included</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />GitHub repo link added</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Pros &amp; Cons table completed</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" checked="checked" />Takeaways section summarized</li>
</ul>

<p><em>Note: Remember to replace <code class="language-plaintext highlighter-rouge">your_groq_api_key</code> with your actual API key and ensure you have sufficient quota for embeddings generation.</em></p>]]></content><author><name>StatNativ Team</name></author><category term="machine-learning" /><category term="ann" /><category term="vector-search" /><category term="algorithms" /><summary type="html"><![CDATA[HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs]]></summary></entry></feed>