<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Transformers - Statnativ</title>
<meta name="description" content="title: Transformers date: 2024-01-02 12:00:00 categories:[AI, Text Processing] tags: [ML, AI, GenAI] author: “Dev” —">


  <meta name="author" content="Amit Tiwari">
  
  <meta property="article:author" content="Amit Tiwari">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Statnativ">
<meta property="og:title" content="Transformers">
<meta property="og:url" content="http://localhost:4000/transformers/">


  <meta property="og:description" content="title: Transformers date: 2024-01-02 12:00:00 categories:[AI, Text Processing] tags: [ML, AI, GenAI] author: “Dev” —">





  <meta name="twitter:site" content="@statnativ">
  <meta name="twitter:title" content="Transformers">
  <meta name="twitter:description" content="title: Transformers date: 2024-01-02 12:00:00 categories:[AI, Text Processing] tags: [ML, AI, GenAI] author: “Dev” —">
  <meta name="twitter:url" content="http://localhost:4000/transformers/">

  
    <meta name="twitter:card" content="summary">
    
  

  



  <meta property="article:published_time" content="2025-01-02T00:00:00+05:30">





  

  


<link rel="canonical" href="http://localhost:4000/transformers/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Amit Tiwari",
      "url": "http://localhost:4000/",
      "sameAs": ["https://www.linkedin.com/in/amit-tiwari-77576248/","https://twitter.com/statnativ","https://github.com/statnativ"]
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Statnativ Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Statnativ
          <span class="site-subtitle">Daily insights on AI, Systems, and Scaling Real-World Products</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/">Home</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/newsletter/">Newsletter</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/profile.jpg" alt="Amit Tiwari" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Amit Tiwari</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Exploring AI, Systems, and Real-World Product Scaling</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">India</span>
        </li>
      

      
        
          
            <li><a href="https://www.linkedin.com/in/amit-tiwari-77576248/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://statnativ.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
            <li><a href="https://twitter.com/statnativ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://github.com/statnativ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:statnativ@gmail.com">
            <meta itemprop="email" content="statnativ@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Transformers">
    <meta itemprop="description" content="title: Transformersdate: 2024-01-02 12:00:00categories:[AI, Text Processing]tags: [ML, AI, GenAI]author: “Dev”—">
    <meta itemprop="datePublished" content="2025-01-02T00:00:00+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Transformers
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        
<hr />
<p>title: Transformers
date: 2024-01-02 12:00:00
categories:[AI, Text Processing]
tags: [ML, AI, GenAI]
author: “Dev”
—</p>

<p><img src="/assets/images/attention_research.png" alt="Attention Research" /></p>

<p><strong><em>Text Generation with Transformers: Fine-Tuning for Specific Tasks</em></strong></p>

<p>Transformers have revolutionized Natural Language Processing (NLP), achieving state-of-the-art results in various tasks, including text generation. Unlike recurrent neural networks (RNNs), transformers process entire sequences in parallel, enabling them to capture long-range dependencies more effectively. This blog post dives into fine-tuning a pre-trained transformer model for a specific text generation task.</p>

<p><strong>What are Transformers?</strong></p>

<p>At their core, transformers rely on the mechanism of <em>self-attention</em>. Self-attention allows the model to weigh the importance of different words in a sentence when processing 1 a particular word. This helps the model understand the context and relationships between words.  </p>

<p><a href="https://hackernews-kappa.vercel.app/best/35977891">1. hackernews-kappa.vercel.app</a></p>

<p><a href="https://hackernews-kappa.vercel.app/best/35977891">hackernews-kappa.vercel.app</a></p>

<p>DeepSeek’s transformer architecture introduced several key innovations that differentiate it from standard transformer models like those used in GPT series. Here are the main architectural differences:</p>

<ol>
  <li>
    <p><strong>Multi-Query Attention (MQA)</strong>: Instead of using multiple sets of key and value projections per attention head (as in multi-head attention), DeepSeek uses a shared key and value projection across all heads while maintaining separate query projections. This significantly reduces computational requirements and memory usage while maintaining performance.</p>
  </li>
  <li>
    <p><strong>RoPE (Rotary Positional Embedding)</strong>: DeepSeek employs rotary positional embeddings instead of absolute or learned positional embeddings. RoPE encodes position information directly into the attention computation through a rotation matrix, which helps the model better understand relative positions and extends more effectively to sequences longer than those seen during training.</p>
  </li>
  <li>
    <p><strong>Grouped-Query Attention (GQA)</strong>: An intermediate approach between multi-head and multi-query attention where key and value projections are shared among groups of attention heads rather than completely shared or completely separate. This offers a balance between computational efficiency and representational power.</p>
  </li>
  <li>
    <p><strong>SwiGLU Activation</strong>: DeepSeek uses SwiGLU activation functions in the feed-forward networks instead of traditional ReLU or GELU activations. SwiGLU combines gating mechanisms with swish activations to improve gradient flow and model capacity.</p>
  </li>
  <li>
    <p><strong>Normalization Placement</strong>: DeepSeek implements “pre-normalization” where layer normalization is applied before the self-attention and feed-forward blocks rather than after them. This pre-norm design helps stabilize training for very deep models.</p>
  </li>
  <li>
    <p><strong>Parallel Structure</strong>: The model uses a parallel structure where the residual connections bypass both the attention and feed-forward blocks in parallel rather than in series, which can improve gradient flow during training.</p>
  </li>
  <li>
    <p><strong>Larger Context Windows</strong>: DeepSeek models were trained with significantly larger context windows than earlier models, enabling them to process and reason over much longer sequences of text.</p>
  </li>
</ol>

<p>These architectural choices collectively allow DeepSeek models to achieve high performance while maintaining computational efficiency, especially for long-context reasoning tasks.</p>

<p><strong>Why Fine-Tuning?</strong></p>

<p>Pre-trained transformer models, like BERT, GPT, and RoBERTa, have learned rich language representations from massive datasets. Fine-tuning these models on a smaller, task-specific dataset allows us to adapt their knowledge to our specific needs without training from scratch, which is computationally expensive and requires vast amounts of data.</p>

<p><strong>Fine-Tuning for Text Generation: A Practical Example</strong></p>

<p>Let’s consider the task of generating creative text, specifically short poems. We will use the <code class="language-plaintext highlighter-rouge">transformers</code> library from Hugging Face, which provides easy access to pre-trained models and fine-tuning tools.</p>

<p><strong>1. Installation:</strong></p>

<p>First, install the necessary libraries:</p>

<p>Bash</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install transformers datasets accelerate
</code></pre></div></div>

<p><strong>2. Loading a Pre-trained Model and Tokenizer:</strong></p>

<p>We’ll use GPT-2 as our base model:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
</code></pre></div></div>

<p><strong>3. Preparing the Dataset:</strong></p>

<p>For this example, let’s assume you have a text file (<code class="language-plaintext highlighter-rouge">poems.txt</code>) where each line is a short poem. You’ll need to tokenize the text:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from datasets import load_dataset
</code></pre></div></div>

<p><strong>4. Fine-Tuning the Model:</strong></p>

<p>Now comes the core part: fine-tuning. We’ll use the <code class="language-plaintext highlighter-rouge">Trainer</code> class from the <code class="language-plaintext highlighter-rouge">transformers</code> library:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import TrainingArguments, Trainer
</code></pre></div></div>

<p><strong>5. Generating Text:</strong></p>

<p>After fine-tuning, you can generate new poems:</p>

<p>Python</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prompt = "The moon shines bright,"
</code></pre></div></div>

<p><strong>Key Parameters for Generation:</strong></p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">max_length</code>: The maximum length of the generated text.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">num_return_sequences</code>: The number of different sequences to generate.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">temperature</code>: Controls the randomness of the generation. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 1.0) make it more creative.</p>
  </li>
</ul>

<p><strong>Conclusion:</strong></p>

<p>Fine-tuning transformers is a powerful technique for adapting pre-trained models to specific text generation tasks. By using the <code class="language-plaintext highlighter-rouge">transformers</code> library, you can easily experiment with different models and datasets to achieve impressive results. Remember to adjust the hyperparameters and training arguments to optimize for your specific use case. This blog post provides a basic introduction; further exploration of advanced techniques like beam search and different decoding strategies can further enhance your text generation capabilities.</p>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2025-01-02T00:00:00+05:30">January 2, 2025</time></p>


      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?via=statnativ&text=Transformers%20http%3A%2F%2Flocalhost%3A4000%2Ftransformers%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Ftransformers%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Ftransformers%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/machine-learning/hnsw-fast-approximate-nearest-neighbors-with-hierarchical-graphs/" class="pagination--pager" title="HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs
">Previous</a>
    
    
      <a href="/Quantization/" class="pagination--pager" title="Quantization
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Quantization/" rel="permalink">Quantization
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machine-learning/hnsw-fast-approximate-nearest-neighbors-with-hierarchical-graphs/" rel="permalink">HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">HNSW: Fast Approximate Nearest Neighbors with Hierarchical Graphs

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://www.linkedin.com/in/amit-tiwari-77576248/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://twitter.com/statnativ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/statnativ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Amit Tiwari. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
